{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FederatedLearningRepro.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mlNPckKJcGSA"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPm4vi2Pu/TBZUSWJN+ML1S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harikuts/Otter.Work/blob/master/FederatedLearningRepro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cwDfXm8MvnQ",
        "colab_type": "text"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This notebook contains the reproduction of results of the original paper on federated learning.\n",
        "\n",
        "## Plan\n",
        "\n",
        "The roadmap for development is as follows:\n",
        "*   Construct standard MNIST example.\n",
        "*   To be continued.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyPkp1yZz527",
        "colab_type": "text"
      },
      "source": [
        "# Standard MNIST\n",
        "\n",
        "There are baseline implementations of a standard example of MNIST. A Keras implementation staands as the first example, but we will port this over to Tensorflow as it provides more low-level functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlNPckKJcGSA",
        "colab_type": "text"
      },
      "source": [
        "## Example 1: Keras Implementation\n",
        "\n",
        "A standard MNIST example from Keras (https://keras.io/examples/mnist_cnn/) is used as a basis to compare our fedeerated model to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZTFxB_IMMGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "import pdb\n",
        "\n",
        "# Configuration\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# pdb.set_trace()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYdxSH2Oalkv",
        "colab_type": "text"
      },
      "source": [
        "## Example 2.0: Tensorflow Implementation (Easy)\n",
        "\n",
        "We begin by using a tutorial provided by Tensorflow (https://www.tensorflow.org/tutorials/quickstart/beginner). It already happens to use the MNIST example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D12oVhOOc1gp",
        "colab_type": "code",
        "outputId": "49c8df02-2d3e-4302-8d65-9513ad373e5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "# Import MNIST data\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Create model\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "# Predictions\n",
        "predictions = model(x_train[:1]).numpy()\n",
        "# Softmax\n",
        "tf.nn.softmax(predictions).numpy()\n",
        "\n",
        "# Defining the loss function\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "loss_fn(y_train[:1], predictions).numpy()\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
        "\n",
        "# Fit model\n",
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 6s 96us/sample - loss: 0.2977 - acc: 0.9121\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 5s 90us/sample - loss: 0.1462 - acc: 0.9565\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 5s 90us/sample - loss: 0.1088 - acc: 0.9677\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 5s 90us/sample - loss: 0.0893 - acc: 0.9730\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 6s 92us/sample - loss: 0.0740 - acc: 0.9767\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2f52425fe465>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce4iHoknf_cE",
        "colab_type": "code",
        "outputId": "a6f95f24-47e1-419d-e9e3-ae1b4b89a600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# print(model.get_weights()[0].shape)\n",
        "# print(model.get_weights()[1].shape)\n",
        "# print(model.get_weights()[2].shape)\n",
        "# print(model.get_weights()[3].shape)\n",
        "\n",
        "import numpy as np\n",
        "a = np.array([1, 2, 3, 4])\n",
        "\n",
        "b = a + a\n",
        "b = sum([a,a])\n",
        "print(b)\n",
        "b = b / 2\n",
        "print(b)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 4 6 8]\n",
            "[1. 2. 3. 4.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXAJn845_4rV",
        "colab_type": "text"
      },
      "source": [
        "# Federated Mode Simulations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0WV97Iovulq",
        "colab_type": "text"
      },
      "source": [
        "## Learning Instance Approach (defunct)\n",
        "\n",
        "In this section, a beginning model is made that describes the higher level behavior of data interactions. I abandoned this model to pursue a simulation closer to the network level."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X5oARzu_1WB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "import random\n",
        "import pdb\n",
        "\n",
        "# Configuration\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# Federated configuration\n",
        "num_clients = 4\n",
        "num_server_rounds = 8\n",
        "num_client_rounds = 2\n",
        "\n",
        "# mnist_train = tfds.load(name=\"mnist\", split=\"train\")\n",
        "# mnist_train = mnist_train.repeat().shuffle(1024).batch(32)\n",
        "# mnist_train = mnist_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "# mnist_test, info = tfds.load(\"mnist\", split=\"test\", with_info=True)\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Splitting the dataset for different clients\n",
        "nonIID = True\n",
        "if nonIID:\n",
        "  percentageMarkers = []\n",
        "  for i in range(num_clients-1):\n",
        "    percentageMarkers.append(random.random())\n",
        "  percentageMarkers.append(1.0)\n",
        "  percentageMarkers = sorted(percentageMarkers)\n",
        "else:\n",
        "  percentageMarkers = [1/num_clients * (n+1) for n in range(num_clients)]\n",
        "\n",
        "# pdb.set_trace()\n",
        "\n",
        "client_x_trains = []\n",
        "client_y_trains = []\n",
        "xMarkers = [int(marker * len(x_train)) for marker in percentageMarkers]\n",
        "yMarkers = [int(marker * len(y_train)) for marker in percentageMarkers]\n",
        "# pdb.set_trace()\n",
        "for j in range(len(percentageMarkers)):\n",
        "  client_x_trains.append(x_train[(xMarkers[j-1] if j > 0 else 0):xMarkers[j]])\n",
        "  client_y_trains.append(x_train[(yMarkers[j-1] if j > 0 else 0):yMarkers[j]])\n",
        "\n",
        "# pdb.set_trace()\n",
        "\n",
        "# Model creation function\n",
        "def createCNN():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                  activation='relu',\n",
        "                  input_shape=(1, 28, 28)))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "  model.compile(\n",
        "            optimizer=keras.optimizers.Adadelta(),\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['sparse_categorical_accuracy'])\n",
        "  return model\n",
        "\n",
        "def createNN():\n",
        "  model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "# Global model initialization\n",
        "global_model = createCNN()\n",
        "global_model.fit(x_train, y_train, epochs=epochs)\n",
        "first_model_done = False\n",
        "\n",
        "\n",
        "# Server action\n",
        "for server_round in range(num_server_rounds):\n",
        "  print(\"SERVER ROUND \", server_round, \":\")\n",
        "  # Clients' actions\n",
        "  for client in range(num_clients):\n",
        "    print(\"\\tCLIENT \", client, \"...\")\n",
        "    # Accept the global model\n",
        "    if first_model_done:\n",
        "      global_weights = global_model.get_weights()\n",
        "    local_weights = []\n",
        "    # Per each round\n",
        "    for client_round in range(num_client_rounds):\n",
        "      print(\"\\t\\tRound \", client_round)\n",
        "      # Train on the local model\n",
        "      round_model = createCNN()\n",
        "      if first_model_done:\n",
        "        round_model.set_weights(global_weights)\n",
        "      # round_model.fit(client_x_trains[client], client_y_trains[client],\n",
        "      #     batch_size=batch_size,\n",
        "      #     epochs=epochs,\n",
        "      #     verbose=1,\n",
        "      #     validation_data=(x_test, y_test))\n",
        "      # local_weights.append(round_model.get_weights())\n",
        "      first_model_done = True\n",
        "    pdb.set_trace()\n",
        "      \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucr9Yx4a8Qtu",
        "colab_type": "text"
      },
      "source": [
        "## Network Model Approach\n",
        "\n",
        "Here we use nodes to carry models. The reason for doing this to prevent the instantiation of new models each time weights have to be transferred. Instead, the state of each model can be preserved in the node that it resides in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WJjvFBBEYs3",
        "colab_type": "code",
        "outputId": "e46454cc-0d1a-4529-df06-13262a03702f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import pdb\n",
        "\n",
        "# Used to start execution ASAP\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "# Configuration\n",
        "num_clients = 4\n",
        "num_epochs = 5\n",
        "num_server_rounds = 2\n",
        "num_client_rounds = 2\n",
        "nonIID = False\n",
        "print (\"Configuration:\" + \\\n",
        "       \"\\n\\t%d clients.\" % (num_clients) + \\\n",
        "       \"\\n\\t%d training epochs.\" % (num_epochs)  + \\\n",
        "       \"\\n\\tUsing %sIID data.\" % (\"non-\" if nonIID else \"\"))\n",
        "\n",
        "# Server class\n",
        "class Server:\n",
        "  def __init__(self, modelGenerator):\n",
        "    self.model = modelGenerator()\n",
        "    self.clients = []\n",
        "    self.neighbors = []\n",
        "# Client class\n",
        "class Client:\n",
        "  def __init__(self, modelGenerator):\n",
        "    self.model = modelGenerator()\n",
        "    self.neighbors = []\n",
        "    self.x_data = None\n",
        "    self.y_data = None\n",
        "  def train(self):\n",
        "    self.model.fit(self.x_data, self.y_data, epochs=num_epochs)\n",
        "# NN model generator function\n",
        "def createNN():\n",
        "  model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10)\n",
        "  ])\n",
        "  loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
        "  return model\n",
        "# Weight averaging\n",
        "def averageWeights(weightsList):\n",
        "  denominator = len(weightsList)\n",
        "  new_weights = []\n",
        "  for part in range(len(weightsList[0])):\n",
        "    part_stack = [weights[part] for weights in weightsList]\n",
        "    new_stack = sum(part_stack) / denominator\n",
        "    new_stack = np.array(new_stack)\n",
        "    new_weights.append(new_stack)\n",
        "  return new_weights\n",
        "\n",
        "# Create the network\n",
        "print (\"\\nCreating a network...\")\n",
        "server = Server(createNN)\n",
        "for i in range(num_clients):\n",
        "  server.clients.append(Client(createNN))\n",
        "\n",
        "# Import MNIST data\n",
        "print (\"\\nDownloading MNIST data...\")\n",
        "mnist = tf.keras.datasets.mnist\n",
        "# Load data into trains\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Splitting the dataset for different clients\n",
        "print (\"\\nSplitting data into different clients...\")\n",
        "if nonIID:\n",
        "  print (\"\\tRandomly assigning ranges of data...\")\n",
        "  percentageMarkers = []\n",
        "  for i in range(num_clients-1):\n",
        "    percentageMarkers.append(random.random())\n",
        "  percentageMarkers.append(1.0)\n",
        "  percentageMarkers = sorted(percentageMarkers)\n",
        "else:\n",
        "  print (\"\\tUniformly assigning ranges of data\")\n",
        "  percentageMarkers = [1/num_clients * (n+1) for n in range(num_clients)]\n",
        "# Storing each subset of data in a client\n",
        "print (\"\\tStoring subsets of data into each client...\")\n",
        "xMarkers = [int(marker * len(x_train)) for marker in percentageMarkers]\n",
        "yMarkers = [int(marker * len(y_train)) for marker in percentageMarkers]\n",
        "for j in range(len(percentageMarkers)):\n",
        "  server.clients[j].x_data = x_train[(xMarkers[j-1] if j > 0 else 0):xMarkers[j]]\n",
        "  server.clients[j].y_data = y_train[(yMarkers[j-1] if j > 0 else 0):yMarkers[j]]\n",
        "\n",
        "# Client data diagnostic\n",
        "print (\"\\nFinished setting up client data!\")\n",
        "for client in server.clients:\n",
        "  print (\"\\tClient %d:\\tX: %d\\tY: %d\" % (server.clients.index(client), len(client.x_data), len(client.y_data)))\n",
        "\n",
        "# list1 = []\n",
        "# list1.append(np.array([1, 2, 3, 4]))\n",
        "# list1.append(np.array([0, 4]))\n",
        "# list2 = []\n",
        "# list2.append(np.array([3, 2, 1, 0]))\n",
        "# list2.append(np.array([4, 0]))\n",
        "# print(averageWeights([list1, list2]))\n",
        "\n",
        "# Server action\n",
        "for server_round in range(num_server_rounds):\n",
        "  print(\"\\nSERVER ROUND \", server_round, \":\\n\")\n",
        "  # Save server model weights\n",
        "  global_weights = server.model.get_weights()\n",
        "  # Clients' actions\n",
        "  client_weight_list = []\n",
        "  for client in server.clients:\n",
        "    # Initialize recorded weights\n",
        "    round_weight_list = []\n",
        "    for client_round in range(num_client_rounds):\n",
        "      # Accept global weights\n",
        "      client.model.set_weights(global_weights)\n",
        "      # Train\n",
        "      client.train()\n",
        "      # Record weights\n",
        "      round_weight_list.append(client.model.get_weights())\n",
        "    client_weight_list.append(averageWeights(round_weight_list))\n",
        "  server.model.set_weights(averageWeights(client_weight_list))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "\t4 clients.\n",
            "\t5 training epochs.\n",
            "\tUsing IID data.\n",
            "\n",
            "Creating a network...\n",
            "\n",
            "Downloading MNIST data...\n",
            "\n",
            "Splitting data into different clients...\n",
            "\tUniformly assigning ranges of data\n",
            "\tStoring subsets of data into each client...\n",
            "\n",
            "Finished setting up client data!\n",
            "\tClient 0:\tX: 15000\tY: 15000\n",
            "\tClient 1:\tX: 15000\tY: 15000\n",
            "\tClient 2:\tX: 15000\tY: 15000\n",
            "\tClient 3:\tX: 15000\tY: 15000\n",
            "\n",
            "SERVER ROUND  0 :\n",
            "\n",
            "Train on 15000 samples\n",
            "Epoch 1/5\n",
            "15000/15000 [==============================] - 2s 143us/sample - loss: 0.5104 - acc: 0.8537\n",
            "Epoch 2/5\n",
            "15000/15000 [==============================] - 1s 88us/sample - loss: 0.2400 - acc: 0.9303\n",
            "Epoch 3/5\n",
            "15000/15000 [==============================] - 1s 89us/sample - loss: 0.1823 - acc: 0.9473\n",
            "Epoch 4/5\n",
            "15000/15000 [==============================] - 1s 84us/sample - loss: 0.1476 - acc: 0.9563\n",
            "Epoch 5/5\n",
            "15000/15000 [==============================] - 1s 85us/sample - loss: 0.1161 - acc: 0.9667\n",
            "Train on 15000 samples\n",
            "Epoch 1/5\n",
            "15000/15000 [==============================] - 1s 89us/sample - loss: 0.4588 - acc: 0.8611\n",
            "Epoch 2/5\n",
            "15000/15000 [==============================] - 1s 88us/sample - loss: 0.2206 - acc: 0.9357\n",
            "Epoch 3/5\n",
            "15000/15000 [==============================] - 1s 90us/sample - loss: 0.1657 - acc: 0.9522\n",
            "Epoch 4/5\n",
            "15000/15000 [==============================] - 1s 91us/sample - loss: 0.1288 - acc: 0.9621\n",
            "Epoch 5/5\n",
            "15000/15000 [==============================] - 1s 90us/sample - loss: 0.1086 - acc: 0.9669\n",
            "Train on 15000 samples\n",
            "Epoch 1/5\n",
            "15000/15000 [==============================] - 2s 112us/sample - loss: 0.5103 - acc: 0.8534\n",
            "Epoch 2/5\n",
            "15000/15000 [==============================] - 1s 88us/sample - loss: 0.2455 - acc: 0.9287\n",
            "Epoch 3/5\n",
            "15000/15000 [==============================] - 1s 94us/sample - loss: 0.1834 - acc: 0.9455\n",
            "Epoch 4/5\n",
            "15000/15000 [==============================] - 1s 97us/sample - loss: 0.1455 - acc: 0.9583\n",
            "Epoch 5/5\n",
            "15000/15000 [==============================] - 1s 95us/sample - loss: 0.1213 - acc: 0.9635\n",
            "Train on 15000 samples\n",
            "Epoch 1/5\n",
            "15000/15000 [==============================] - 1s 96us/sample - loss: 0.4577 - acc: 0.8623\n",
            "Epoch 2/5\n",
            "15000/15000 [==============================] - 1s 92us/sample - loss: 0.2233 - acc: 0.9345\n",
            "Epoch 3/5\n",
            "15000/15000 [==============================] - 1s 96us/sample - loss: 0.1721 - acc: 0.9483\n",
            "Epoch 4/5\n",
            "15000/15000 [==============================] - 1s 91us/sample - loss: 0.1380 - acc: 0.9597\n",
            "Epoch 5/5\n",
            "15000/15000 [==============================] - 1s 95us/sample - loss: 0.1139 - acc: 0.9639\n",
            "Train on 15000 samples\n",
            "Epoch 1/5\n",
            "15000/15000 [==============================] - 2s 111us/sample - loss: 0.5330 - acc: 0.8465\n",
            "Epoch 2/5\n",
            "15000/15000 [==============================] - 1s 90us/sample - loss: 0.2510 - acc: 0.9271\n",
            "Epoch 3/5\n",
            "15000/15000 [==============================] - 1s 88us/sample - loss: 0.1884 - acc: 0.9451\n",
            "Epoch 4/5\n",
            "15000/15000 [==============================] - 1s 86us/sample - loss: 0.1496 - acc: 0.9559\n",
            "Epoch 5/5\n",
            "15000/15000 [==============================] - 1s 88us/sample - loss: 0.1233 - acc: 0.9628\n",
            "Train on 15000 samples\n",
            "Epoch 1/5\n",
            "15000/15000 [==============================] - 1s 87us/sample - loss: 0.4677 - acc: 0.8562\n",
            "Epoch 2/5\n",
            "15000/15000 [==============================] - 1s 85us/sample - loss: 0.2319 - acc: 0.9316\n",
            "Epoch 3/5\n",
            "15000/15000 [==============================] - 1s 87us/sample - loss: 0.1782 - acc: 0.9479\n",
            "Epoch 4/5\n",
            "15000/15000 [==============================] - 1s 87us/sample - loss: 0.1411 - acc: 0.9575\n",
            "Epoch 5/5\n",
            "15000/15000 [==============================] - 1s 92us/sample - loss: 0.1150 - acc: 0.9653\n",
            "Train on 15000 samples\n",
            "Epoch 1/5\n",
            "15000/15000 [==============================] - 2s 108us/sample - loss: 0.4928 - acc: 0.8537\n",
            "Epoch 2/5\n",
            "15000/15000 [==============================] - 1s 87us/sample - loss: 0.2367 - acc: 0.9308\n",
            "Epoch 3/5\n",
            "15000/15000 [==============================] - 1s 90us/sample - loss: 0.1821 - acc: 0.9475\n",
            "Epoch 4/5\n",
            "15000/15000 [==============================] - 1s 89us/sample - loss: 0.1430 - acc: 0.9571\n",
            "Epoch 5/5\n",
            "15000/15000 [==============================] - 1s 87us/sample - loss: 0.1195 - acc: 0.9655\n",
            "Train on 15000 samples\n",
            "Epoch 1/5\n",
            "15000/15000 [==============================] - 1s 85us/sample - loss: 0.4360 - acc: 0.8685\n",
            "Epoch 2/5\n",
            "15000/15000 [==============================] - 1s 90us/sample - loss: 0.2122 - acc: 0.9379\n",
            "Epoch 3/5\n",
            "15000/15000 [==============================] - 1s 92us/sample - loss: 0.1608 - acc: 0.9529\n",
            "Epoch 4/5\n",
            "15000/15000 [==============================] - 1s 87us/sample - loss: 0.1296 - acc: 0.9616\n",
            "Epoch 5/5\n",
            "15000/15000 [==============================] - 1s 90us/sample - loss: 0.1065 - acc: 0.9689\n",
            "\n",
            "SERVER ROUND  1 :\n",
            "\n",
            "Train on 15000 samples\n",
            "Epoch 1/5\n",
            "15000/15000 [==============================] - 1s 94us/sample - loss: 0.1478 - acc: 0.9558\n",
            "Epoch 2/5\n",
            "15000/15000 [==============================] - 1s 91us/sample - loss: 0.1108 - acc: 0.9677\n",
            "Epoch 3/5\n",
            "15000/15000 [==============================] - 1s 89us/sample - loss: 0.0907 - acc: 0.9745\n",
            "Epoch 4/5\n",
            "15000/15000 [==============================] - 1s 90us/sample - loss: 0.0705 - acc: 0.9805\n",
            "Epoch 5/5\n",
            "15000/15000 [==============================] - 1s 87us/sample - loss: 0.0605 - acc: 0.9820\n",
            "Train on 15000 samples\n",
            "Epoch 1/5\n",
            "15000/15000 [==============================] - 1s 91us/sample - loss: 0.1475 - acc: 0.9581\n",
            "Epoch 2/5\n",
            "15000/15000 [==============================] - 1s 87us/sample - loss: 0.1098 - acc: 0.9677\n",
            "Epoch 3/5\n",
            "15000/15000 [==============================] - 1s 92us/sample - loss: 0.0875 - acc: 0.9735\n",
            "Epoch 4/5\n",
            "15000/15000 [==============================] - 1s 87us/sample - loss: 0.0727 - acc: 0.9787\n",
            "Epoch 5/5\n",
            "15000/15000 [==============================] - 1s 88us/sample - loss: 0.0576 - acc: 0.9831\n",
            "Train on 15000 samples\n",
            "Epoch 1/5\n",
            "15000/15000 [==============================] - 1s 90us/sample - loss: 0.1499 - acc: 0.9539\n",
            "Epoch 2/5\n",
            "15000/15000 [==============================] - 1s 89us/sample - loss: 0.1124 - acc: 0.9659\n",
            "Epoch 3/5\n",
            "15000/15000 [==============================] - 1s 90us/sample - loss: 0.0923 - acc: 0.9714\n",
            "Epoch 4/5\n",
            "15000/15000 [==============================] - 1s 91us/sample - loss: 0.0746 - acc: 0.9759\n",
            "Epoch 5/5\n",
            "15000/15000 [==============================] - 2s 104us/sample - loss: 0.0659 - acc: 0.9802\n",
            "Train on 15000 samples\n",
            "Epoch 1/5\n",
            "15000/15000 [==============================] - 1s 97us/sample - loss: 0.1520 - acc: 0.9539\n",
            "Epoch 2/5\n",
            "15000/15000 [==============================] - 1s 89us/sample - loss: 0.1152 - acc: 0.9650\n",
            "Epoch 3/5\n",
            "15000/15000 [==============================] - 1s 92us/sample - loss: 0.0910 - acc: 0.9726\n",
            "Epoch 4/5\n",
            "15000/15000 [==============================] - 1s 94us/sample - loss: 0.0733 - acc: 0.9777\n",
            "Epoch 5/5\n",
            "15000/15000 [==============================] - 1s 92us/sample - loss: 0.0645 - acc: 0.9808\n",
            "Train on 15000 samples\n",
            "Epoch 1/5\n",
            "15000/15000 [==============================] - 1s 94us/sample - loss: 0.1532 - acc: 0.9555\n",
            "Epoch 2/5\n",
            "15000/15000 [==============================] - 1s 95us/sample - loss: 0.1189 - acc: 0.9631\n",
            "Epoch 3/5\n",
            "15000/15000 [==============================] - 1s 88us/sample - loss: 0.0967 - acc: 0.9699\n",
            "Epoch 4/5\n",
            "15000/15000 [==============================] - 1s 92us/sample - loss: 0.0816 - acc: 0.9762\n",
            "Epoch 5/5\n",
            "15000/15000 [==============================] - 1s 88us/sample - loss: 0.0694 - acc: 0.9789\n",
            "Train on 15000 samples\n",
            "Epoch 1/5\n",
            "15000/15000 [==============================] - 1s 88us/sample - loss: 0.1578 - acc: 0.9529\n",
            "Epoch 2/5\n",
            "15000/15000 [==============================] - 1s 90us/sample - loss: 0.1146 - acc: 0.9649\n",
            "Epoch 3/5\n",
            "15000/15000 [==============================] - 1s 91us/sample - loss: 0.0970 - acc: 0.9715\n",
            "Epoch 4/5\n",
            "15000/15000 [==============================] - 1s 89us/sample - loss: 0.0789 - acc: 0.9763\n",
            "Epoch 5/5\n",
            "15000/15000 [==============================] - 1s 88us/sample - loss: 0.0661 - acc: 0.9798\n",
            "Train on 15000 samples\n",
            "Epoch 1/5\n",
            "15000/15000 [==============================] - 1s 90us/sample - loss: 0.1490 - acc: 0.9570\n",
            "Epoch 2/5\n",
            "15000/15000 [==============================] - 1s 91us/sample - loss: 0.1136 - acc: 0.9662\n",
            "Epoch 3/5\n",
            "15000/15000 [==============================] - 1s 89us/sample - loss: 0.0923 - acc: 0.9719\n",
            "Epoch 4/5\n",
            "15000/15000 [==============================] - 1s 92us/sample - loss: 0.0751 - acc: 0.9773\n",
            "Epoch 5/5\n",
            "15000/15000 [==============================] - 1s 89us/sample - loss: 0.0650 - acc: 0.9803\n",
            "Train on 15000 samples\n",
            "Epoch 1/5\n",
            "15000/15000 [==============================] - 1s 90us/sample - loss: 0.1502 - acc: 0.9555\n",
            "Epoch 2/5\n",
            "15000/15000 [==============================] - 1s 97us/sample - loss: 0.1121 - acc: 0.9670\n",
            "Epoch 3/5\n",
            "15000/15000 [==============================] - 1s 96us/sample - loss: 0.0896 - acc: 0.9731\n",
            "Epoch 4/5\n",
            "15000/15000 [==============================] - 1s 94us/sample - loss: 0.0751 - acc: 0.9774\n",
            "Epoch 5/5\n",
            "15000/15000 [==============================] - 1s 94us/sample - loss: 0.0613 - acc: 0.9815\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}